{"cells":[{"metadata":{"colab_type":"text","id":"6AD6qcnKCGJ_"},"cell_type":"markdown","source":"# Support Vector Machines\n\nWelcome to your next lab! You will build SVM algorithm and explore working of this model with different kernels.\n\n\n**You will learn to:**\n- Build the general architecture of a learning algorithm with OOP in mind:\n    - Helper functions:\n        - Kernels\n        - Kernels matrix\n        - Computing lagrange multipliers\n        - Extracting support features\n        \n    - Main Model Class:\n        - Training\n        - Prediction "},{"metadata":{"colab_type":"text","id":"fq_EilhNCGKA"},"cell_type":"markdown","source":"## 1 - Packages ##\n\nFirst, let's run the cell below to import all the packages that you will need during this assignment. \n- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n- [cvxopt](http://cvxopt.org) is a software package for convex optimization."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"mcO6FkOpCGKC","trusted":true},"cell_type":"code","source":"import numpy as np\nimport cvxopt\nimport matplotlib.pyplot as plt\n\ncvxopt.solvers.options['show_progress'] = False","execution_count":3,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'cvxopt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-a4a877a9257d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcvxopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcvxopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolvers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'show_progress'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cvxopt'"]}]},{"metadata":{"colab_type":"text","id":"-qxKCR9sCGKF"},"cell_type":"markdown","source":"## 2 - Overview of the Dataset  ##\n\n**Problem Statement**: You are given a dataset  containing:\n    - a training set of m_train examples\n    - a test set of m_test examples\n    - each example is of shape (number of features, 1)\n    \n  This dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like \"leaflets three, let it be'' for Poisonous Oak and Ivy."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"_g_O-5jmCGKF","trusted":true},"cell_type":"code","source":"def load_data():\n    from sklearn.model_selection import train_test_split\n    \n    X = np.genfromtxt('mush_features.csv')\n    Y = np.genfromtxt('mush_labels.csv')\n    \n    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(X, Y, test_size=0.33, random_state=42)\n    \n    train_set_x = train_set_x[:300].astype(float)\n    train_set_y = train_set_y[:300].astype(float)\n    \n    test_set_x = test_set_x[:100].astype(float)\n    test_set_y = test_set_y[:100].astype(float)\n    \n    x_test = train_set_x[:5]\n    y_test = train_set_y[:5]   \n    \n    train_set_x = train_set_x.reshape(train_set_x.shape[0], -1).T\n    test_set_x = test_set_x.reshape(test_set_x.shape[0], -1).T\n    \n    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))\n    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))\n    \n    x_test = x_test.reshape(x_test.shape[0], -1).T\n    y_test = y_test.reshape((1, y_test.shape[0]))\n    \n    return train_set_x, test_set_x, train_set_y, test_set_y, x_test, y_test","execution_count":4,"outputs":[]},{"metadata":{"colab_type":"text","id":"NTETwuB0CGKI"},"cell_type":"markdown","source":"Many software bugs in machine learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.\n\nSo, let's check shapes:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"9pbP4st-CGKJ","trusted":true},"cell_type":"code","source":"train_set_x, test_set_x, train_set_y, test_set_y, x_test, y_test = load_data()\nprint('train_set_x.shape: ', train_set_x.shape)\nprint('test_set_x.shape: ', test_set_x.shape)\nprint('train_set_y.shape: ', train_set_y.shape)\nprint('test_set_y.shape: ', test_set_y.shape)","execution_count":5,"outputs":[{"output_type":"stream","text":"train_set_x.shape:  (22, 300)\ntest_set_x.shape:  (22, 100)\ntrain_set_y.shape:  (1, 300)\ntest_set_y.shape:  (1, 100)\n","name":"stdout"}]},{"metadata":{"colab_type":"text","id":"V58fhdj2CGKL"},"cell_type":"markdown","source":"**Expected Output for m_train, m_test**: \n<table style=\"width:30%\">\n  <tr>\n      <td><b>train_set_x.shape:</b></td>\n    <td> (22, 300) </td> \n  </tr>\n  \n  <tr>\n    <td><b>test_set_x.shape:</b></td>\n    <td> (22, 100) </td> \n  </tr>\n  \n  <tr>\n    <td><b>train_set_y.shape:</b></td>\n    <td> (1,300) </td> \n  </tr>\n  \n  <tr>\n    <td><b>test_set_y.shape:</b></td>\n    <td> (1,100) </td> \n  </tr>\n  \n\n  \n</table>"},{"metadata":{"colab_type":"text","id":"WkSQzJgFCGKM"},"cell_type":"markdown","source":"Distribution of samples in train set:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"6L4ActVBCGKN","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(4, 3))\nplt.hist(train_set_y.T)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"4isdygxzCGKP"},"cell_type":"markdown","source":"Distribution of samples in test set:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"_hfyRUPRCGKQ","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(4, 3))\nplt.hist(test_set_y.T)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"LwMM05R-CGKT"},"cell_type":"markdown","source":"## 3 - General Architecture of the learning algorithm ##"},{"metadata":{"colab_type":"text","id":"_BVOA877CGKU"},"cell_type":"markdown","source":"The SVM algorithm is implemented in practice using a kernel.\n\nThe learning of the hyperplane in linear SVM is done by transforming the problem using some linear algebra.\n\nA powerful insight is that the linear SVM can be rephrased using the inner product of any two given observations, rather than the observations themselves. The inner product between two vectors is the sum of the multiplication of each pair of input values.\n\nFor example, the inner product of the vectors [2, 3] and [5, 6] is 28.\n\nThe equation for making a prediction for a new input using the dot product between the input ($x$) and each support vector ($x_s$) is calculated as follows: $$f(x) = sign(b + \\sum_{s \\in S}(a_s * k(x, x_s)))$$\n\nThis is an equation that involves calculating the inner products of a new input vector ($x$) with all support vectors in training data. The coefficients $b$ and $a_i$ (for each input) must be estimated from the training data by the learning algorithm. `S` is a set of all support vectors and `k` is a kernel."},{"metadata":{"colab_type":"text","id":"Ymp4asNcCGKU"},"cell_type":"markdown","source":"### 3.1 - Kernels###\n\n\n"},{"metadata":{"colab_type":"text","id":"4VIrD-uzCGKV"},"cell_type":"markdown","source":"#### Linear Kernel\n\nThe dot-product is called the linear kernel and can be re-written as:\n\n$$k(x_{i},x _{j}) = x_{i}x_{j}\\tag{1}$$\n\nThe kernel defines the similarity or a distance measure between new data and the support vectors. The dot product is the similarity measure used for linear SVM or a linear kernel because the distance is a linear combination of the inputs.\n\nIt is desirable to use more complex kernels as it allows lines to separate the classes that are curved or even more complex. This in turn can lead to more accurate classifiers."},{"metadata":{"colab_type":"text","id":"jWmhG4F4CGKW"},"cell_type":"markdown","source":"#### Polynomial Kernel SVM ####\nInstead of the dot-product, we can use a polynomial kernel, for example:\n\n$$k(x_{i},x _{j}) = (x_{i}x_{j} + coef)^{d}\\tag{2}$$\n\nWhere the degree of the polynomial must be specified additionally to the learning algorithm. When d=1 this is the same as the linear kernel. The polynomial kernel allows for curved lines in the input space."},{"metadata":{"colab_type":"text","id":"XJWQNcnoCGKZ"},"cell_type":"markdown","source":"#### Gaussian radial basis function (RBF) ####\n$$k(x_{i}, x_{j})=exp(-\\gamma {\\left \\| x_i-x_j \\right \\|^2})\\tag{4}$$"},{"metadata":{"colab_type":"text","id":"JWn04erTCGKZ"},"cell_type":"markdown","source":"Implement these kernel functions in the cell below:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"AMrAvicqCGKa","trusted":true},"cell_type":"code","source":"# GRADED CLASS: Kernel\n\nclass Kernel(object):\n    def linear():\n        \"\"\"\n            Returns:\n            function that takes two vectors as a parameters and returns their dot product\n        \"\"\"\n        ### START CODE HERE ###\n        return lambda x, y: np.dot(x, y)\n        ### END CODE HERE ###\n        \n    def polynomial(coef, power):\n        \"\"\"\n            Arguments:\n            coef: float\n            power: int\n        \n            Returns:\n            function that takes two vectors as a parameters and computes polynomial kernel\n        \"\"\"\n        ### START CODE HERE ### (≈ 1 line of code)\n        return lambda x, y: (np.dot(x, y) + coef) ** power\n        ### END CODE HERE ###\n        \n    def rbf(gamma):\n        \"\"\"\n            Arguments:\n            gamma: float\n        \n            Returns:\n            function that takes two vectors as a parameters and computes rbf kernel\n        \"\"\"\n        ### START CODE HERE ### (≈ 1-2 line of code)\n        return lambda x, y: np.exp(-gamma * np.linalg.norm(x - y) ** 2)\n        ### END CODE HERE ###","execution_count":6,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"6qwELuymCGKc","trusted":true},"cell_type":"code","source":"lin = Kernel.linear()\npolynom = Kernel.polynomial(3, 2)\nrbf = Kernel.rbf(0.0002)\n\nxt = x_test[:, 1]\nyt = x_test[:, 2]\n\nprint('linear_kernel: ', lin(xt, yt))\nprint('polynomial_kernel: ', polynom(xt, yt))\nprint('rbf_kernel: ', rbf(xt, yt))","execution_count":7,"outputs":[{"output_type":"stream","text":"linear_kernel:  202.0\npolynomial_kernel:  42025.0\nrbf_kernel:  0.9906440418940348\n","name":"stdout"}]},{"metadata":{"colab_type":"text","id":"VeUYwV5PCGKf"},"cell_type":"markdown","source":"**Expected Output**: \n<table style=\"width:40%\">\n  <tr>\n      <td><b>linear_kernel:</b></td>\n    <td> 202.0 </td> \n  </tr>\n  \n  <tr>\n    <td><b>polynomial_kernel:</b></td>\n    <td>42025.0 </td> \n  </tr>\n  \n  <tr>\n    <td><b>rbf_kernel:</b></td>\n    <td> 0.9906440418940348 </td> \n  </tr>\n\n  \n\n  \n</table>"},{"metadata":{"colab_type":"text","id":"sQ8CW1GtCGKg"},"cell_type":"markdown","source":"### 3.2 Model"},{"metadata":{"colab_type":"text","id":"ebPo_0aVCGKg"},"cell_type":"markdown","source":"In the main class you need to implement all the funcionallity:\n1. `_kernel_matrix` - calculate kernel matrix\n    - Get number of samples\n    - Create zero matrix of quadratic shape of number of samples\n    - Calculate kernels\n2. `_compute_lagrange_multipliers` - solve a quadratic optimization problem and compute lagrange multipliers\n    - Get number of samples\n    - Create Kernel matrix by calling `_kernel_matrix` function\n    - Create create quadratic term P based on Kernel matrix\n    - Create linear term q\n    - Create G, h, A, b\n    - Solve with - cvxopt.solvers.qp(P, q, G, h, A, b)\n    - Return flatten vector of lagrange multipliers\n3. `_get_support_vectors` - extract support vectors\n    - Get non-zero lagrange multipliers indicies\n    - Get he corresponding lagrange multipliers\n    - Get support vecorts\n    - Get the samples that will act as support vectors\n    - Get the corresponding labels\n4. `fit` - compute b and lagrange multipliers\n    - Solve a quadratic optimization problem and compute lagrange multipliers by calling `_compute_lagrange_multipliers`\n    - Extract support vectors and non zero lagrange multipliers by calling `_get_support_vectors`\n    - Calculate $b$ using first support vector: $b = y_{s0} - \\sum_{s \\in S} a_s * y_s * k(x_{s0}, x_s)$, where $S$ is a set of all support vectors\n5. `predict` - use trained by `fit` params to make predictions: $f(x) = sign(b + \\sum_{s \\in S}(a_s * y_s*k(x, x_s)))$"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"5rjAuJ2ZCGKh","trusted":true},"cell_type":"code","source":"# GRADED CLASS: SVM\n\nclass SVM(object):\n    \"\"\"\n    The Support Vector Machines classifier\n\n    Arguments:\n    C -- penalty term\n    kernel -- kernel function e.g. lambda x, y: ...\n    \"\"\"\n\n    def __init__(self, C=1, kernel=Kernel.linear()):\n        self.C = C\n        self.kernel = kernel\n        self.non_zero_multipliers = None\n        self.support_vectors = None\n        self.support_labels = None\n        self.b = None\n\n    def _kernel_matrix(self, X):\n        \"\"\"\n        Computes kernel matrix applying kernel function pairwise for each sample\n\n        Arguments:\n        X -- input matrix of shape (number of features, number of samples)\n\n        Returns:\n        kernels matrix of shape (number of samples, number of samples)\n        \"\"\"\n        ### START CODE HERE ###\n        # Get number of samples\n        n_samples = X.shape[1]\n\n        # Calculate kernels pairwise and fill kernels matrix\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                x = X[:, i]\n                y = X[:, j]\n                K[i][j] += self.kernel(x, y)\n        # Return kernel matrix\n        return K\n        ### END CODE HERE ###\n\n    def _compute_lagrange_multipliers(self, X, Y):\n        \"\"\"\n        Solves the quadratic optimization problem and calculates lagrange multipliers\n\n        We need to solve\n            min 1/2 x^T P x + q^T x (aplha is x)\n        s.t.\n            Gx <= h (alpha >= 0)\n            Ax = b (y^T * alpha = 0)\n\n        Arguments:\n        X -- input matrix of shape (number of features, number of samples)\n        Y -- labels of shape (1, number of samples)\n\n        Returns:\n        numpy array of lagrange multipliers\n        \"\"\"\n        ### START CODE HERE ###\n        # Get number of samples\n        n_samples = X.shape[1]\n\n        # Get Kernel matrix by calling _kernel_matrix function\n        K = self._kernel_matrix(X)\n\n        # Create create quadratic term P based on Kernel matrix\n        P = cvxopt.matrix(np.outer(Y, Y) * K)\n\n        # Create linear term q\n        q = cvxopt.matrix(np.ones(n_samples) * -1)\n\n        # Create G, h\n        if not self.C:\n            G = cvxopt.matrix(np.identity(n_samples) * -1)\n            h = cvxopt.matrix(np.zeros(n_samples))\n        else:\n            G_max = np.identity(n_samples) * -1\n            G_min = np.identity(n_samples)\n\n            G = cvxopt.matrix(np.vstack((G_max, G_min)))\n\n            h_max = cvxopt.matrix(np.zeros(n_samples))\n            h_min = cvxopt.matrix(np.ones(n_samples) * self.C)\n\n            h = cvxopt.matrix(np.vstack((h_max, h_min)))\n\n        # Create A, b\n        A = cvxopt.matrix(Y, (1, n_samples))\n        b = cvxopt.matrix(0.0)\n\n        # Solve the quadratic optimization problem using cvxopt\n        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n\n        # Extract flat array of lagrange multipliers\n        lagrange_multipliers = np.ravel(solution['x'])\n\n        # Return lagrange multipliers\n        return lagrange_multipliers\n        ### END CODE HERE ###\n\n    def _get_support_vectors(self, lagrange_multipliers, X, Y):\n        \"\"\"\n        Extracts the samples that will act as support vectors and corresponding labels\n\n        Arguments:\n        lagrange_multipliers -- numpy array of lagrange multipliers\n        X -- input matrix of shape (number of features, number of samples)\n        Y -- labels of shape (1, number of samples)\n\n        Returns:\n        non_zero_multipliers -- numpy array of non-zero lagrange multipiers (>1e-7)\n        support_vectors -- matrix of support vectors of shape (number of features, number of support vectors)\n        support_vector_labels -- corresponding labels of shape (1, number of support vectors)\n        \"\"\"\n        ### START CODE HERE ###\n        # Get indexes of non-zero lagrange multipiers\n        idx = lagrange_multipliers > 1e-7\n\n        # Get the corresponding lagrange multipliers\n        non_zero_multipliers = lagrange_multipliers[idx]\n\n        # Get the samples that will act as support vectors\n        support_vectors = X[:, idx]\n\n        # Get the corresponding labels\n        support_labels = Y[:, idx]\n\n        # Return\n        return non_zero_multipliers, support_vectors, support_labels\n        ### END CODE HERE ###\n\n    def fit(self, X, Y):\n        \"\"\"\n        Main training function\n\n        Arguments:\n        X -- input matrix of shape (number of features, number of samples)\n        Y -- labels of shape (1, number of samples)\n        \"\"\"\n        ### START CODE HERE ###\n        # Solve the quadratic optimization problem and get lagrange multipliers\n        lagrange_multipliers = self._compute_lagrange_multipliers(X, Y)\n\n\n        # Extract support vectors and non zero lagrange multipliers\n        self.non_zero_multipliers, self.support_vectors, self.support_labels = self._get_support_vectors(\n            lagrange_multipliers, X, Y)\n\n        # Calculate b using first support vector\n        self.b = self.support_labels[0, 0] - np.sum(self.non_zero_multipliers * self.support_labels * self.kernel(self.support_vectors[:, 0], self.support_vectors))\n        ### END CODE HERE ###\n\n    def predict(self, X):\n        \"\"\"\n        Predict function\n\n        Arguments:\n        X -- input matrix of shape (number of features, number of samples)\n\n        Returns:\n        predictions of shape (1, number of samples)\n        \"\"\"\n        ### START CODE HERE ###\n        n_samples = X.shape[1]\n        predictions = []\n\n        for i in range(n_samples):\n            predictions.append(np.sign(self.b + np.sum(self.non_zero_multipliers * self.support_labels * self.kernel(X[:, i], self.support_vectors))))\n\n        return np.asarray(predictions).reshape((1, n_samples))\n        ### END CODE HERE ###\n","execution_count":13,"outputs":[]},{"metadata":{"colab_type":"text","id":"dlWBQHcLCGKj"},"cell_type":"markdown","source":"Initialize model with default linear term and penalty term = 2"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"WwPn-CCKCGKk","trusted":true},"cell_type":"code","source":"model = SVM(C=2)","execution_count":9,"outputs":[]},{"metadata":{"colab_type":"text","id":"EOWq1UZuCGKo"},"cell_type":"markdown","source":"Kernel matrix check"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"BBTeyrmCCGKp","trusted":true},"cell_type":"code","source":"k = model._kernel_matrix(x_test)\nprint('kernel matrix:')\nprint(k)","execution_count":10,"outputs":[{"output_type":"stream","text":"kernel matrix:\n[[240. 231. 190. 145. 180.]\n [231. 248. 202. 152. 201.]\n [190. 202. 203. 135. 197.]\n [145. 152. 135. 109. 129.]\n [180. 201. 197. 129. 255.]]\n","name":"stdout"}]},{"metadata":{"colab_type":"text","id":"HzpCx9K3CGKr"},"cell_type":"markdown","source":"**Expected Output**: \n<table style=\"width:35%\">\n  <tr>\n      <td style=\"width:15%\"><b>kernel matrix:</b></td>\n    <td> [[ 240.  231.  190.  145.  180.]<br>\n [ 231.  248.  202.  152.  201.]<br>\n [ 190.  202.  203.  135.  197.]<br>\n [ 145.  152.  135.  109.  129.]<br>\n [ 180.  201.  197.  129.  255.]]\n      </td> \n  </tr>\n  \n  \n</table>"},{"metadata":{"colab_type":"text","id":"TmJMFbnmCGKs"},"cell_type":"markdown","source":"Lagrange multipliers check"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"zLnnaU_HCGKt","trusted":true},"cell_type":"code","source":"lm = model._compute_lagrange_multipliers(x_test, y_test)\nprint('lagrange multipliers: ')\nprint(lm)","execution_count":11,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'cvxopt' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-78683665f2c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_lagrange_multipliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lagrange multipliers: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-bac054460754>\u001b[0m in \u001b[0;36m_compute_lagrange_multipliers\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Create create quadratic term P based on Kernel matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvxopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Create linear term q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'cvxopt' is not defined"]}]},{"metadata":{"colab_type":"text","id":"s2aWEWUXCGKv"},"cell_type":"markdown","source":"**Expected Output**: \n<table style=\"width:30%\">\n  <tr style=\"width:10%\">\n      <td><b>lagrange multipliers:</b></td>\n    <td> [0.00540158<br>0.02154727<br>0.06545855<br>0.05981161<br>0.03259579]\n      </td> \n  </tr>\n  \n  \n</table>"},{"metadata":{"colab_type":"text","id":"LyH1rbe0CGKw"},"cell_type":"markdown","source":"Support vector extraction check, remember that support vectors are columns"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"jDnKniWICGKx","trusted":true},"cell_type":"code","source":"nzl, sv, sl = model._get_support_vectors(lm, x_test,y_test)\nprint('non-zero lagrange multipliers:')\nprint(nzl)\nprint('support vectors:')\nprint(sv)\nprint('support labels:')\nprint(sl)\n","execution_count":12,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'lm' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-27e6b96d8628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnzl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_support_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'non-zero lagrange multipliers:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnzl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'support vectors:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'lm' is not defined"]}]},{"metadata":{"colab_type":"text","id":"pFKzgbYLCGKz"},"cell_type":"markdown","source":"**Expected Output for m_train, m_test**: \n<table style=\"width:50%\">\n    <tr>\n    <td><b>non-zero lagrange multipliers:</b></td>\n    <td> [ 0.00540158<br>  0.02154727<br>  0.06545855<br>  0.05981161<br>  0.03259579]\n      </td> \n  </tr>\n  <tr>\n      <td><b>support vectors:</b></td>\n    <td> [[5. 5. 2. 2. 2.]<br>\n [3. 3. 2. 0. 3.]<br>\n [4. 2. 4. 3. 2.]<br>\n [1. 1. 0. 0. 0.]<br>\n [5. 5. 5. 2. 7.]<br>\n [1. 1. 1. 1. 1.]<br>\n [0. 0. 1. 0. 0.]<br>\n [0. 0. 0. 0. 1.]<br>\n [7. 5. 4. 3. 0.]<br>\n [1. 1. 1. 0. 1.]<br>\n [1. 1. 3. 1. 0.]<br>\n [2. 2. 2. 1. 1.]<br>\n [2. 2. 0. 1. 1.]<br>\n [7. 7. 7. 6. 7.]<br>\n [3. 7. 7. 4. 7.]<br>\n [0. 0. 0. 0. 0.]<br>\n [2. 2. 2. 2. 2.]<br>\n [1. 1. 1. 1. 1.]<br>\n [4. 4. 0. 2. 0.]<br>\n [3. 2. 3. 1. 7.]<br>\n [4. 5. 3. 4. 4.]<br>\n [0. 0. 1. 1. 4.]]\n      </td> \n  </tr>\n    <tr>\n        <td><b>support labels:</b></td>\n    <td> [[-1. -1. -1.  1.  1.]]\n      </td> \n  </tr>\n\n</table>"},{"metadata":{"colab_type":"text","id":"cSiyER-vCGK0"},"cell_type":"markdown","source":"Definition of accuracy metrics for classification"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"1xMASksOCGK0","trusted":true},"cell_type":"code","source":"def accuracy(predictions, labels):\n    return np.sum(predictions == labels, axis=1) / float(labels.shape[1])","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"tragEwQFCGK2"},"cell_type":"markdown","source":"## 4 - Training"},{"metadata":{"colab_type":"text","id":"TmEDh9zBCGK2"},"cell_type":"markdown","source":"Firstly, let's initialize our classifier:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"w975mCf_CGK3","trusted":true},"cell_type":"code","source":"clf = SVM(C=1, kernel=Kernel.linear())","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"ydaaLmGqCGK5"},"cell_type":"markdown","source":"And, finaly, train"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"CcZXpq0DCGK6","trusted":true},"cell_type":"code","source":"clf.fit(train_set_x, train_set_y)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"yv2Bi7w1CGK9"},"cell_type":"markdown","source":"## 5 - Making predictions"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"bUw4kuNICGK9","trusted":true},"cell_type":"code","source":"y_pred = clf.predict(test_set_x)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"lXuMrR3WCGK_"},"cell_type":"markdown","source":"Let's calculate accuracy (accuracy of model must be >0.97):"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"_FgVusl6CGLA","trusted":true},"cell_type":"code","source":"accuracy(y_pred, test_set_y)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"crkEu7-BCGLC"},"cell_type":"markdown","source":"## 6 - Visualization\n\nNow let's generate some simple data to see how types of kernel affects the decision boundary."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"dRI0xVPfCGLD","trusted":true},"cell_type":"code","source":"samples = np.random.normal(size=200).reshape(2, 100)\nlabels = (2 * (samples.sum(axis=0) > 0) - 1.0).reshape(1, 100)","execution_count":0,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"ZxdK1tyCCGLF","trusted":true},"cell_type":"code","source":"def plot(model, X, Y, grid_size):\n    \n    import matplotlib.cm as cm\n    import itertools\n    \n    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n    \n    xx, yy = np.meshgrid(\n        np.linspace(x_min, x_max, grid_size),\n        np.linspace(y_min, y_max, grid_size),\n        indexing='ij'\n    )\n    \n    flatten = lambda m: np.array(m).reshape(-1,)\n\n    result = []\n\n    model.fit(X, Y)\n    \n    for (i, j) in itertools.product(range(grid_size), range(grid_size)):\n        point = np.array([[xx[i, j]], [yy[i, j]]])\n        result.append(model.predict(point)[0, 0])\n\n    print(np.array(result).shape)\n    print(xx.shape)\n    \n    Z = np.array(result).reshape(xx.shape)\n    \n    plt.contourf(\n        xx, yy, Z,\n        cmap=cm.Paired,\n        levels=[-0.01, 0.01],\n        extend='both',\n        alpha=0.7\n    )\n    \n    \n    plt.scatter(\n        flatten(X[0, :]),\n        flatten(X[1, :]),\n        c=flatten(Y),\n        cmap=cm.Paired,\n    )\n    \n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n    plt.show()","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"aMRPDFe_CGLH"},"cell_type":"markdown","source":"#### Linear Kernel"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"s63dfOrxCGLH","trusted":true},"cell_type":"code","source":"clf_lin = SVM(C=1, kernel=Kernel.linear())\nplot(clf_lin, samples, labels, 200)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"U1HHtRFBCGLO"},"cell_type":"markdown","source":"#### Polynomial Kernel"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"Eyo9CuwtCGLP","trusted":true},"cell_type":"code","source":"clf_polynomial = SVM(C=1, kernel=Kernel.polynomial(1, 3))\nplot(clf_polynomial, samples, labels, 200)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"I85NUBFXCGLS"},"cell_type":"markdown","source":"#### RBF"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"I-JnwlIvCGLT","trusted":true},"cell_type":"code","source":"clf_rbf = SVM(C=1, kernel=Kernel.rbf(0.03))\nplot(clf_rbf, samples, labels, 200)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"94bJ-18ZCGLb"},"cell_type":"markdown","source":"## 7 - Conclusion\nAs we can see, our model fits well the hypothesis function to the data.\n\n#### What's next:\n1. Try experimenting with the kernel parameters to see how this affects the model you have built.\n2. Compare the results you have obtained with the `sklearn.svm.SVC` model.\n3. Try this model in the wild! Select your favorite dataset [here](https://www.kaggle.com/datasets?sortBy=hottest&group=public&page=1&pageSize=20&size=small&filetype=all&license=all&tagids=13303) and play with it."}],"metadata":{"colab":{"collapsed_sections":[],"default_view":{},"name":"SVM.ipynb","provenance":[],"version":"0.3.2","views":{}},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
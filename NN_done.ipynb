{"cells":[{"metadata":{"colab_type":"text","id":"GfMODGu7rcQS"},"cell_type":"markdown","source":"# Neural Network\n\nWelcome to your next lab! You will solve problem of handwritten digits recognition using Neural Network.\n\n\n**You will learn to:**\n- Build the general architecture of a learning algorithm with OOP in mind:\n    - Helper utilities\n        - Sigmoid (and it's derivative)\n        - One-Hot\n        - Cost Function\n        - Regularization\n    - Neural Network Class\n        - Forward propagation\n        - Backward propagation\n        - Upgrade parameters\n    - Main Model Classes\n        - Training\n        - Prediction "},{"metadata":{"colab_type":"text","id":"YZHxDPk4rcQT"},"cell_type":"markdown","source":"## 1 - Packages ##\n\nFirst, let's run the cell below to import all the packages that you will need during this assignment. \n- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n- [seaborn](https://seaborn.pydata.org/) is a Python visualization library which provides a high-level interface for drawing attractive statistical graphics."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"ORs0sBnKrcQU","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set(style='whitegrid', palette='muted', font_scale=1.5)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"1dN7h9uyrcQW"},"cell_type":"markdown","source":"## 2 - Overview of the Problem set ##\n\n**Problem Statement**: \nWe'll use the MNIST data set, which contains tens of thousands of scanned images of handwritten digits, together with their correct classifications. MNIST's name comes from the fact that it is a modified subset of two data sets collected by NIST, the United States' National Institute of Standards and Technology.\n\nThe MNIST dataset contains 60,000 images. These images are scanned handwriting samples from 250 people, half of whom were US Census Bureau employees, and half of whom were high school students. The images are greyscale and 28 by 28 pixels in size. \nSo, you are given a dataset  containing:\n    - a training set of m_train examples labeld as 0-9\n    - a test set of m_test examples labeld as 0-9\n    - each example is an array of length 784 (28 * 28) which represents image of handwritten digit.\n    \nYou will build an algorithm that can recognize handwritten digits.\n\nLet's get more familiar with the dataset. Load the data by running the following code."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"beyv9EkQrcQY","trusted":true},"cell_type":"code","source":"# Read the MNIST dataset from ubyte files \n\ndef read_mnist(images_path, labels_path):\n    import struct\n    import os\n    with open(labels_path, 'rb') as p:\n        magic, n = struct.unpack('>II', p.read(8))\n        labels = np.fromfile(p, dtype=np.uint8)\n    with open(images_path, 'rb') as p:\n        magic, num, rows, cols = struct.unpack(\">IIII\", p.read(16))\n        images = np.fromfile(p, dtype=np.uint8).reshape(len(labels), 784)\n\n    return images, labels\n\n# Shuffle dataset\n\ndef shuffle_data(features, labels, random_seed=42):\n    assert len(features) == len(labels)\n\n    if random_seed:\n        np.random.seed(random_seed)\n    idx = np.random.permutation(len(features))\n    return [a[idx] for a in [features, labels]] \n\n# Loading data\n\ndef load_data():     \n    X, y = read_mnist('samples/train-images-idx3-ubyte', 'samples/train-labels-idx1-ubyte')\n    X, y = shuffle_data(X, y, random_seed=42)\n    train_set_x, train_set_y = X[:5000], y[:5000]\n    test_set_x, test_set_y = X[5000:], y[5000:]\n    \n    test_set_x = test_set_x.reshape(test_set_x.shape[0], -1).T\n    train_set_x = train_set_x.reshape(train_set_x.shape[0], -1).T\n    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))\n    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))\n    \n    return train_set_x, test_set_x, train_set_y, test_set_y","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"ubqWjSy-rcQa"},"cell_type":"markdown","source":"Let's create train and test datasets:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"qjxlgf9zrcQa","trusted":true},"cell_type":"code","source":"train_set_x, test_set_x, train_set_y, test_set_y = load_data()\nprint('train set shapes: ', train_set_x.shape, train_set_y.shape)\nprint('test set shapes: ', test_set_x.shape, test_set_y.shape)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"NOlYBZ0OrcQc"},"cell_type":"markdown","source":"**Expected Output**: \n\n<table style=\"width:30%\">\n    <tr>\n        <td><b>train set shapes:</b></td>\n       <td> (784, 5000) <br>(1, 5000)</td>\n    </tr>\n    <tr>\n        <td><b>test set shapes:</b></td>\n       <td> (784, 55000)<br> (1, 55000)</td>\n    </tr>\n</table>"},{"metadata":{"colab_type":"text","id":"skWPmakQrcQd"},"cell_type":"markdown","source":"### Data exploration ###\n\nLet's build a function to check how the data looks like:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"6QbZbLC-rcQd","trusted":true},"cell_type":"code","source":"def plot_digit(x_set, y_set, idx):\n    img = x_set.T[idx].reshape(28,28)\n    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n    plt.title('true label: %d' % y_set.T[idx])\n    plt.show()","execution_count":0,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"TAhiHIFPrcQf","trusted":true},"cell_type":"code","source":"plot_digit(train_set_x, train_set_y, idx=1)","execution_count":0,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"Vu2pein_rcQh","trusted":true},"cell_type":"code","source":"plot_digit(train_set_x, train_set_y, idx=3)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"FjTJ3vlXrcQk"},"cell_type":"markdown","source":"## 3 - Helper Functions\n\nFor begining we need to implement some special functions.\n\n### Sigmoid (and it's derivative)\n\nAny layer of a neural network can be considered as an Affine Transformation followed by application of a non linear function. A vector is received as input and is multiplied with a matrix to produce an output , to which a bias vector may be added before passing the result through an activation function such as sigmoid.\n\n$$Input = x \\quad Output = f(Wx+b)\\tag{1}$$\n\nThe sigmoid function is used quite commonly in the realm of deep learning, at least it was until recently. It has distinct **S** shape and it is a differentiable real function for any real input value. Additionally, it has a positive derivative at each point. More importantly, we will use it as an activation function for the hidden layer of our model. Here's how it is defined:\n\n$$\\sigma (x) = \\frac{1}{1+e^{-x}}\\tag{2}$$\n\nHere is first derivative (which we will use during the backpropagation step of our training algorithm). It has the following formula:\n\n$$\\frac{d\\sigma (x)}{d(x)} = \\sigma (x)\\cdot (1-\\sigma(x))\\tag{3}$$"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"g-d2fr5ZrcQk","trusted":true},"cell_type":"code","source":"# GRADED CLASS: Sigmoid\n\nclass Sigmoid:\n    def __call__(self, z):\n        \"\"\"\n        Compute the sigmoid of z\n\n        Arguments:\n        z -- scalar or numpy array of any size.\n\n        Return:\n        sigmoid(z)\n        \"\"\"\n        ### START CODE HERE ### (≈ 1 line of code)\n        return 1 / (1 + np.exp(-z))\n        ### END CODE HERE ###\n    \n    def prime(self, z):\n        \"\"\"\n        Compute the derivative of sigmoid of z\n\n        Arguments:\n        z -- scalar or numpy array of any size.\n\n        Return:\n        Sigmoid prime\n        \"\"\"\n        ### START CODE HERE ### (≈ 1-2 lines of code)\n        return self.__call__(z) * (1 - self.__call__(z))\n\n        ### END CODE HERE ###","execution_count":0,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"rBFlF48xrcQn","trusted":true},"cell_type":"code","source":"x = np.linspace(-10., 10., num=100)\nsig = Sigmoid()(x)\nsig_prime = Sigmoid().prime(x)\nprint('sigmoid:')\nprint(sig[:5])\nprint('sigmoid_prime: ')\nprint(sig_prime[:5])","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"viMgshR3rcQo"},"cell_type":"markdown","source":"**Expected Output**: \n\n<table style=\"width:35%\">\n    <tr>\n       <td><b>sigmoid:</b></td>\n       <td> [  4.53978687e-05<br>   5.55606489e-05<br>   6.79983174e-05   8.32200197e-05<br>\n   1.01848815e-04] </td>\n    </tr>\n    <tr>\n        <td><b>sigmoid_prime:</b></td>\n       <td> [  4.53958077e-05<br>   5.55575620e-05<br>   6.79936937e-05<br>   8.32130942e-05<br>\n   1.01838442e-04] </td>\n    </tr>\n\n</table>"},{"metadata":{"colab_type":"text","id":"tmZftWXmrcQp"},"cell_type":"markdown","source":"Now let's look at the picture:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"um0eqOTmrcQp","trusted":true},"cell_type":"code","source":"plt.plot(x, sig, label=\"sigmoid\")\nplt.plot(x, sig_prime, label=\"sigmoid prime\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(prop={'size' : 16})\nplt.show()","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"jBDfFhGrrcQr"},"cell_type":"markdown","source":"The derivative shows the rate of change of a function. We can use it to determine the \"slope\" of that function. The highest rate of change for the sigmoid function is when $x=0$, as it is evident from the derivative graph."},{"metadata":{"colab_type":"text","id":"rA8DZ8hLrcQs"},"cell_type":"markdown","source":"### One-Hot ###\n\nFor convenience let's represent input labels $y$ as a 10-dimensional vector. For example, if a particular training image, $x$, depicts a 6, then $y(x)=(0,0,0,0,0,0,1,0,0,0)^T$ is the desired output from the network.\n\nSo, let's encode our labels as one-hot vectors:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"INLlyecGrcQt","trusted":true},"cell_type":"code","source":"# GRADED FUNCTION: one_hot\n\ndef one_hot(Y, n_classes):\n    \"\"\"\n    Encode labels into a one-hot representation\n\n    Arguments:\n    Y -- array of input labels of shape (1, n_samples)\n    n_classes -- number of classes\n\n    Returns:\n    onehot, a matrix of labels by samples. For each column, the ith index will be \n        \"hot\", or 1, to represent that index being the label; shape - (n_classes, n_samples)\n    \"\"\"\n    ### START CODE HERE ### (≈ 1-4 lines of code)\n    one_h = np.zeros((n_classes, Y.shape[1]))\n    for i in range(Y.shape[1]):\n        for j in range(n_classes):\n            if Y[0][i] == j:\n                one_h[j][i] = 1\n    return one_h\n    \n    ### END CODE HERE ###","execution_count":0,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"fNLPz3NxrcQv","trusted":true},"cell_type":"code","source":"print('encoded:')\nprint(one_hot(np.asarray([1, 2, 3, 4, 3]).reshape(1, 5), 5))","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"JgU13t4PrcQw"},"cell_type":"markdown","source":"**Expected Output**: \n\n<table style=\"width:30%\">\n    <tr>\n        <td><b>encoded:</b></td>\n        <td> [[ 0.  0.  0.  0.  0.]<br>\n [ 1.  0.  0.  0.  0.]<br>\n [ 0.  1.  0.  0.  0.]<br>\n [ 0.  0.  1.  0.  1.]<br>\n [ 0.  0.  0.  1.  0.]]</td>\n    </tr>\n    "},{"metadata":{"colab_type":"text","id":"R5uVa8VCrcQx"},"cell_type":"markdown","source":"### Cost Function ###\n\nHere is cost function for our model:\n$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\sum\\limits_{j = 0}^{n}\\large{(} \\small y^{(i)}_{j}\\log\\left(a^{[2] (i)}_{j}\\right) + (1-y^{(i)}_{j})\\log\\left(1- a^{[2] (i)}_{j}\\right)  \\small\\large{)}\\tag{4}$$\nwhere \n\n$m$ - number of examples;\n\n$n$ - number of classes;\n\n$y$ - input labels;\n\n$a$ - output of hidden layer.\n\n\nWe will use cross entropy loss:\n$- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})\\tag{5}$"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"toLXsObJrcQy","trusted":true},"cell_type":"code","source":"# GRADED FUNCTION: compute_cost\n\ndef compute_cost(A2, Y):\n    \"\"\"\n    Computes the cross-entropy cost given in equation (4)\n    \n    Arguments:\n    A2 -- sigmoid output of the hidden layer activation, of shape (classes, n_examples)\n    Y -- labels of shape (classes, n_examples)\n    \n    Returns:\n    cost -- cross-entropy cost given equation (4)\n    \"\"\"\n    \n    m = Y.shape[1] # number of examples\n\n    # Compute the cross-entropy cost\n    ### START CODE HERE ###\n    cost = -(1 / m) * np.sum(np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)))\n   \n    ### END CODE HERE ###\n    \n    return cost","execution_count":0,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"copZTxSercQ0","trusted":true},"cell_type":"code","source":"np.random.seed(1)\nlabels = (np.random.randn(2, 3) > 0)\na_h = (np.array([[ 0.5 ,  0.49,  0.5], [0.1 ,  0.2,  0.3]]))\nprint(\"cost:\" + str(compute_cost(a_h, labels)))","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"awY8OGb9rcQ2"},"cell_type":"markdown","source":"**Expected Output**: \n\n<table style=\"width:25%\">\n    <tr>\n       <td><b>cost:</b></td>\n       <td> 1.3770374288... </td>\n    </tr>\n    \n\n</table>"},{"metadata":{"colab_type":"text","id":"bY-ZSMGercQ2"},"cell_type":"markdown","source":"### Regularization\n\nIn order to create less complex (parsimonious) model when you have a large number of features in your dataset, some of the Regularization techniques used to address over-fitting and feature selection are:\n\n1. L1 Regularization\n\n2. L2 Regularization\n\nA regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.\n\nLasso Regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the cost function.\n\nRidge regression adds “squared magnitude” of coefficient as penalty term to the loss function. \nThe cost is then computed by summing squared diff over all training examples.\n$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\sum\\limits_{j = 0}^{n}\\large{(} \\small y^{(i)}_{j}\\log\\left(a^{[2] (i)}_{j}\\right) + (1-y^{(i)}_{j})\\log\\left(1- a^{[2] (i)}_{j}\\right)  \\small\\large{)}+$$<br>$$+\\frac{\\lambda_1}{m}( ||w_1||_1 + ||w_2||_1) + \\frac{\\lambda_2}{2m} (||w_1||_2^2+||w_2||_2^2) \\tag{6}$$\n\nTo calculate norms use \"entrywise\" norm."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"40GkU8w4rcQ2","trusted":true},"cell_type":"code","source":"# GRADED CLASS: Regularisation\n\nclass Regularization:\n    \"\"\" \n    Regularization class\n\n    Arguments:\n    lambda_1 -- regularization coeficient for l1 regularization\n    lambda_2 -- regularization coeficient for l2 regularization\n    \"\"\"\n    def __init__(self, lambda_1, lambda_2):\n        self.lambda_1 = lambda_1\n        self.lambda_2 = lambda_2\n        \n        \n    def l1(self, W1, W2, m):\n        \"\"\" \n        Compute l1 regularization part\n\n        Arguments:\n        W1 -- weigts of shape (n_hidden_units, n_features) \n        W2 -- weigts of shape (output_size, n_hidden_units) \n        m -- n_examples\n\n        Returns:\n        l1_term -- float, check formula (6)\n        \"\"\"\n        ### START CODE HERE ###\n        return (self.lambda_1 / m) * np.sum(np.abs(W1) + np.abs(W2))\n       \n        ### END CODE HERE ###\n        \n    def l1_grad(self, W1, W2, m):\n        \"\"\" \n        Compute l1 regularization term\n\n        Arguments:\n        W1 -- weigts of shape (n_hidden_units, n_features) \n        W2 -- weigts of shape (output_size, n_hidden_units) \n        m -- n_examples\n\n        Returns:\n         dict with l1_grads \"dW1\" and \"dW2\"\n            which are grads by corresponding weights\n        \"\"\"\n        ### START CODE HERE ###\n        l1_grad_w1 = (self.lambda_1 / m) * np.sign(W1)\n        l1_grad_w2 = (self.lambda_1 / m) * np.sign(W2)\n        dict_l1 = {'dW1': l1_grad_w1, 'dW2': l1_grad_w2}\n        return dict_l1\n        \n        ### END CODE HERE ###\n\n    def l2(self, W1, W2, m):\n        \"\"\" \n        Compute l2 regularization term\n\n        Arguments:\n        W1 -- weigts of shape (n_hidden_units, n_features) \n        W2 -- weigts of shape (output_size, n_hidden_units) \n        m -- n_examples\n\n        Returns:\n        l2_term: float, check formula (6)\n        \"\"\"\n        ### START CODE HERE ###\n        return (self.lambda_2 / (2 * m)) * (np.linalg.norm(W1) ** 2 + np.linalg.norm(W2) ** 2)\n       \n        ### END CODE HERE ###\n        \n    def l2_grad(self, W1, W2, m):\n        \"\"\" \n        Compute l2 regularization term\n\n        Arguments:\n        W1 -- weigts of shape (n_hidden_units, n_features) \n        W2 -- weigts of shape (output_size, n_hidden_units) \n        m -- n_examples\n\n        Returns:\n        l2_grads: dict with keys \"dW1\" and \"dW2\"\n        \"\"\"\n        ### START CODE HERE ###\n        l2_grad_w1 = (self.lambda_2 / m) * W1\n        l2_grad_w2 = (self.lambda_2 / m) * W2\n        dict_l2 = {'dW1': l2_grad_w1, 'dW2': l2_grad_w2}\n        return dict_l2\n \n        ### END CODE HERE ###","execution_count":0,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"9p0bRXJ5rcQ5","trusted":true},"cell_type":"code","source":"reg = Regularization(0.3, 0.2)\nprint('l1 reg. term =', reg.l1(np.array([0.1, -0.2, 0.3, 0.4]), np.array([-0.5, 0.6, 0.7, 0.8]), 5))\nprint('l2 reg. term =', reg.l2(np.array([0.1, -0.2, 0.3, 0.4]), np.array([-0.5, 0.6, 0.7, 0.8]), 5))\nprint('l1 reg. gradient =', reg.l1_grad(np.array([0.1, -0.2, 0.3, 0.4]), np.array([-0.5, 0.6, 0.7, 0.8]), 5))\nprint('l2 reg. gradient =', reg.l2_grad(np.array([0.1, -0.2, 0.3, 0.4]), np.array([-0.5, 0.6, 0.7, 0.8]), 5))","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"iqVtxccvrcQ7"},"cell_type":"markdown","source":"**Expected Output**: \n\n<table style=\"width:80%\">\n    <tr>\n        <td><b>l1 reg. term</b></td>\n       <td> 0.216 </td>\n    </tr>\n    <tr>\n        <td><b>l2 reg. term</b></td>\n       <td>  0.0408 </td>\n    </tr>\n    <tr>\n        <td><b>l1 reg. gradient</b></td>\n       <td> {'dW1': array([ 0.06, -0.06,  0.06,  0.06]), </td>\n       <td>  'dW2': array([-0.06,  0.06,  0.06,  0.06])} </td>\n    </tr>\n    <tr>\n        <td><b>l2 reg. gradient</b></td>\n       <td> {'dW1': array([ 0.004, -0.008,  0.012,  0.016]),\n  </td>\n      <td> 'dW2': array([-0.02 ,  0.024,  0.028,  0.032])}\n      </td>\n    </tr>\n\n\n</table>"},{"metadata":{"colab_type":"text","id":"Ixbg4LqprcQ8"},"cell_type":"markdown","source":"## 3 - Neural Network. General Architecture ##\n\nTo recognize digits we will use a two-layer neural network. Here is model of our network:\n    <img src=\"samples/arc.png\" style=\"width:620px;height:500px;\">\n    \nThe input layer of the network contains neurons encoding the values of the input pixels. As discussed earlier, training data consist of many 28 by 28 pixel images of scanned handwritten digits, and so the input layer contains $784=28×28$ neurons. The input pixels are greyscale, with a value of 0.0 representing white, a value of 1.0 representing black, and in between values representing gradually darkening shades of grey.\n\nThe first layer of the network is a hidden layer. We denote the number of neurons in this hidden layer by n, and we'll experiment with different values for n. The example shown illustrates a small hidden layer, containing just $n=15$ neurons.\n\nThe output layer of the network contains 10 neurons. If the first neuron fires, i.e., has an output $≈1$, then that will indicate that the network thinks the digit is a 0. If the second neuron fires then that will indicate that the network thinks the digit is a 1, and so on. A little more precisely, we number the output neurons from 0 through 9, and figure out which neuron has the highest activation value. If that neuron is, say, neuron number 6, then our network will guess that the input digit was a 6. And so on for the other output neurons.\n\nEach training input $x$ is a $28×28=784$-dimensional vector. Each entry in the vector represents the grey value for a single pixel in the image. \n"},{"metadata":{"colab_type":"text","id":"MZqGKYMYrcQ9"},"cell_type":"markdown","source":"### 3.1 - Neural Network Class ####\n\nIn this block we will implement class *NeuralNetwork*, which will perform forward and backward propagation, and update parameters.  \n\n**Main steps to build neural network:**\n    1. Initialize the model's parameters\n    2. Loop:\n        - Implement forward propagation\n        - Compute loss\n        - Implement backward propagation to get the gradients\n        - Update parameters (gradient descent)\n        \n**1. Initializing the model's parameters**\n\nYou need to do following steps:\n- Make sure your parameters' sizes are right. Refer to the neural network figure above if needed.        \n- Initialize the weights matrices with random values from the “standard normal” distribution, where sigma = 0.01, mu = 0. \n- Initialize the bias vectors as zeros. \n   \n**2. Loop**\n\n   - *Forward propagation*\n   \nIn forward propagation we use the data and the weights of the network to compute a prediction, so during this step we just multiply the matrix containing our training data with the matrix of the weights of the hidden layer.\n\nFor one example $x^{(i)}$:\n$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{8}$$ \n$$a^{[1] (i)} = \\sigma(z^{[1] (i)})\\tag{9}$$\n$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{10}$$\n$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{11}$$\n$$y^{(i)}_{prediction} = argmax(a^{[2](i)}) \\tag{12}$$\n\nDefinition: \n\n$W^{[1]}$: weights matrix 1\n\n$W^{[2]}$: weights matrix 2\n\n$b^{[1]}$: bias 1\n\n$b^{[2]}$: bias 2\n\n$z^{[1] (i)}$ : input of the hidden layer\n\n$a^{[1] (i)}$: activation of the hidden layer\n\n$z^{[2] (i)}$: input of the output layer \n\n$a^{[2] (i)}$: activation of the output layer\n\n$\\hat{y}^{(i)}$: array of predictions\n\n   - *Backward propagation*\n   \nIn backpropagation step we apply the activation function (sigmoid) to the result and multiply that with the weight matrix of the output layer. \n\n<table style=\"width:90%\">\n    <tr>\n       <td> $dz^{[2]} = a^{[2]} - y$  </td>\n       <td> $dZ^{[2]} = A^{[2]} - Y$ </td>\n    </tr>\n    <tr>\n       <td> $dW^{[2]} = dz^{[2]}a^{[1]T} + \\frac{\\lambda_1}{m}sign(W^{[2]}) + \\frac{\\lambda_2}{m}W^{[2]}$ </td>\n       <td>  $dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]T} + \\frac{\\lambda_1}{m}sign(W^{[2]}) + \\frac{\\lambda_2}{m}W^{[2]}$ </td>\n    </tr>\n    <tr>\n       <td> $db^{[2]} = dz^{[2]}$ </td>\n       <td> $db^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True)$ </td>\n    </tr>\n    <tr>\n       <td> $dz^{[1]} = W^{[2]T}dz^{[2]}*\\sigma'(z^{[1]})$ </td>\n       <td> $dz^{[1]} = W^{[2]T}dZ^{[2]}*\\sigma'(Z^{[1]})$ </td>\n    </tr>\n    <tr>\n       <td> $dW^{[1]} = dz^{[1]}x^T + \\frac{\\lambda_1}{m}sign(W^{[1]}) + \\frac{\\lambda_2}{m}W^{[1]}$ </td>\n       <td> $dW^{[1]} = \\frac{1}{m}dZ^{[1]}X^T + \\frac{\\lambda_1}{m}sign(W^{[1]}) + \\frac{\\lambda_2}{m}W^{[1]}$  </td>\n    </tr>\n    <tr>\n       <td> $db^{[1]} = dZ^{[1]}$ </td>\n       <td> $db^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)$  </td>\n    </tr>\n</table>\n\n   - *Update Parameters*\n   \nNext, the error is computed based on the prediction and the provided labels. The final step propagates the error through the network, starting from the final layer. Thus, the weights get updated based on the error, little by little. To implement this method we will use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\n\n**General gradient descent rule**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter.\n\n**Illustration**: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.\n\n<img src=\"samples/sgd.gif\" style=\"width:400;height:400;\"> <img src=\"samples/sgd_bad.gif\" style=\"width:400;height:400;\">\n\n\nImplement class `NeuralNetwork` in the cell below: "},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"dQox_YlurcQ-","trusted":true},"cell_type":"code","source":"# GRADED CLASS NeuralNetwork\n\nclass NeuralNetwork:\n    \"\"\"\n    Arguments:\n    n_features: int -- Number of features\n    n_hidden_units: int -- Number of hidden units\n    n_classes: int -- Number of classes\n    learning_rate: float\n    reg: instance of Regularization class\n    \"\"\"\n\n    def __init__(self, n_features, n_hidden_units, n_classes, learning_rate, reg=Regularization(0.1, 0.2),\n                 sigm=Sigmoid()):\n        self.n_features = n_features\n        self.n_classes = n_classes\n        self.learning_rate = learning_rate\n        self.n_hidden_units = n_hidden_units\n        self.reg = reg\n        self.sigm = sigm\n        self.W1 = None\n        self.b1 = None\n        self.W2 = None\n        self.b2 = None\n\n        self.initialize_parameters()\n\n    def initialize_parameters(self):\n        \"\"\"\n        W1 -- weight matrix of shape (self.n_hidden_units, self.n_features)\n        b1 -- bias vector of shape (self.n_hidden_units, 1)\n        W2 -- weight matrix of shape (self.n_classes, self.n_hidden_units)\n        b2 -- bias vector of shape (self.n_classes, 1)\n        \"\"\"\n        np.random.seed(42)\n\n        ### START CODE HERE ###\n        self.W1 = 0.01 * np.random.randn(self.n_hidden_units, self.n_features)\n        self.b1 = np.zeros((self.n_hidden_units, 1))\n        self.W2 = 0.01 * np.random.randn(self.n_classes, self.n_hidden_units)\n        self.b2 = np.zeros((self.n_classes, 1))\n\n        ### END CODE HERE ###\n\n    def forward_propagation(self, X):\n        \"\"\"\n        Arguments:\n        X -- input data of shape (number of features, number of examples)\n\n        Returns:\n        dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n        \"\"\"\n        # Implement Forward Propagation to calculate A2 (probabilities)\n        ### START CODE HERE ###\n        Z1 = np.dot(self.W1, X) + self.b1\n        A1 = self.sigm(Z1)\n        Z2 = np.dot(self.W2, A1) + self.b2\n        A2 = self.sigm(Z2)\n        ### END CODE HERE ###\n\n        return {\n            'Z1': Z1,\n            'A1': A1,\n            'Z2': Z2,\n            'A2': A2\n        }\n\n    def backward_propagation(self, X, Y, cache):\n        \"\"\"\n        Arguments:\n        X -- input data of shape (number of features, number of examples)\n        Y -- one-hot encoded vector of labels with shape (n_classes, n_samples)\n        cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n\n        Returns:\n        dictionary containing gradients \"dW1\", \"db1\", \"dW2\", \"db2\"\n        \"\"\"\n        m = X.shape[1]\n\n        # Retrieve also A1 and A2 from dictionary \"cache\".\n        ### START CODE HERE ###\n        A1 = cache['A1']\n        A2 = cache['A2']\n        Z1 = cache['Z1']\n\n        ### END CODE HERE ###\n\n        # Backward propagation: calculate dW1, db1, dW2, db2.\n        ### START CODE HERE ###\n        dZ2 = A2 - Y\n        dW2_l1 = self.reg.l1_grad(self.W1, self.W2, m)['dW2']\n        dW2_l2 = self.reg.l2_grad(self.W1, self.W2, m)['dW2']\n        dW1_l1 = self.reg.l1_grad(self.W1, self.W2, m)['dW1']\n        dW1_l2 = self.reg.l2_grad(self.W1, self.W2, m)['dW1']\n        dW2 = (1 / m) * np.dot(dZ2, A1.T) + dW2_l1 + dW2_l2\n        db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n        dZ1 = np.dot(self.W2.T, dZ2) * self.sigm.prime(Z1)\n        dW1 = (1 / m) * np.dot(dZ1, X.T) + dW1_l1 + dW1_l2\n        db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n\n        ### END CODE HERE ###\n\n        return {\n            'dW1': dW1,\n            'db1': db1,\n            'dW2': dW2,\n            'db2': db2\n        }\n\n    def update_parameters(self, grads):\n        \"\"\"\n        Updates parameters using the gradient descent update rule\n\n        Arguments:\n        grads -- python dictionary containing gradients \"dW1\", \"db1\", \"dW2\", \"db2\"\n        \"\"\"\n        # Retrieve each gradient from the dictionary \"grads\"\n\n        ### START CODE HERE ###\n        dW1 = grads[\"dW1\"]\n        dW2 = grads[\"dW2\"]\n        db1 = grads[\"db1\"]\n        db2 = grads[\"db2\"]\n\n        ## END CODE HERE ###\n\n        # Update each parameter\n        ### START CODE HERE ###\n        self.W1 = self.W1 - self.learning_rate * dW1\n        self.W2 = self.W2 - self.learning_rate * dW2\n        self.b1 = self.b1 - self.learning_rate * db1\n        self.b2 = self.b2 - self.learning_rate * db2\n\n        ### END CODE HERE ###\n","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"OV38UoLjrcRA"},"cell_type":"markdown","source":"Init Neural Network class:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"sN98l0qXrcRB","trusted":true},"cell_type":"code","source":"nn_test = NeuralNetwork(2, 4, 3, 0.01)\nx_test = np.asarray([[1, 2, 1, 1], [2, 1, 2, 1]])\ny_test = np.asarray([1, 2, 1, 0]).reshape(1, 4)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"ymsN6J8HrcRD"},"cell_type":"markdown","source":"Initialize parameters test:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"W8ZrHazfrcRD","trusted":true},"cell_type":"code","source":"print('W1:', nn_test.W1)\nprint('b1:', nn_test.b1)\nprint('W2:', nn_test.W2)\nprint('b2:', nn_test.b2)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"vGxLujVQrcRG"},"cell_type":"markdown","source":"**Expected Output**: \n\n<table style=\"width:65%\">\n    <tr>\n       <td style=\"width:15%\"><b>W1</b></td>\n       <td> [[ 0.00496714 -0.00138264]<br>\n [ 0.00647689  0.0152303 ]<br>\n [-0.00234153 -0.00234137]<br>\n [ 0.01579213  0.00767435]] </td>\n    </tr>\n    <tr>\n        <td><b>b1</b></td>\n       <td> [[ 0.]<br>\n [ 0.]<br>\n [ 0.]<br>\n [ 0.]] </td>\n    </tr>\n    <tr>\n        <td><b>W2</b></td>\n       <td> [[-0.00469474  0.0054256  -0.00463418 -0.0046573 ]<br>\n [ 0.00241962 -0.0191328  -0.01724918 -0.00562288]<br>\n [-0.01012831  0.00314247 -0.00908024 -0.01412304]] </td>\n    </tr>\n    <tr>\n        <td><b>b2</b></td>\n       <td> [[0.]<br>\n [0.]<br>\n [0.]] </td>\n</table>"},{"metadata":{"colab_type":"text","id":"W7cE_v6brcRG"},"cell_type":"markdown","source":"Forward method test:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"YhqZSQOircRH","trusted":true},"cell_type":"code","source":"cache = nn_test.forward_propagation(x_test)\nprint('Z1:',cache['Z1'])\nprint('A1:',cache['A1'])\nprint('Z2:',cache['Z2'])\nprint('A2:',cache['A2'])","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"fSDutx2QrcRI"},"cell_type":"markdown","source":"**Expected Output**: \n\n<table style=\"width:70%\">\n    <tr>\n        <td style=\"width:15%\"><b>Z1</b></td>\n       <td> [[ 0.00220186  0.00855164  0.00220186  0.0035845 ]<br>\n [ 0.03693748  0.02818407  0.03693748  0.02170718]<br>\n [-0.00702427 -0.00702444 -0.00702427 -0.0046829 ]<br>\n [ 0.03114082  0.0392586   0.03114082  0.02346648]] </td>\n    </tr>\n    <tr>\n        <td><b>A1</b></td>\n       <td> [[0.50055046 0.5021379  0.50055046 0.50089612]<br>\n [0.50923332 0.50704555 0.50923332 0.50542658]<br>\n [0.49824394 0.4982439  0.49824394 0.49882928]<br>\n [0.50778458 0.50981339 0.50778458 0.50586635]] </td>\n    </tr>\n    <tr>\n        <td><b>Z2</b></td>\n       <td> [[-0.00426091 -0.00428969 -0.00426091 -0.00427697]<br>\n [-0.01998143 -0.01994713 -0.01998143 -0.01990707]<br>\n [-0.01516511 -0.01521672 -0.01516511 -0.0151588 ]] </td>\n    </tr>\n    <tr>\n        <td><b>A2</b></td>\n       <td> [[0.49893477 0.49892758 0.49893477 0.49893076]<br>\n [0.49500481 0.49501338 0.49500481 0.4950234 ]<br>\n [0.49620879 0.49619589 0.49620879 0.49621037]]</td>\n</table>"},{"metadata":{"colab_type":"text","id":"qR7nh5NCrcRI"},"cell_type":"markdown","source":"Backward method test:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"iWmjuZ5urcRJ","trusted":true},"cell_type":"code","source":"grads = nn_test.backward_propagation(x_test, y_test, cache)\nprint (\"dW1: \"+ str(grads[\"dW1\"]))\nprint (\"db1: \"+ str(grads[\"db1\"]))\nprint (\"dW2: \"+ str(grads[\"dW2\"]))\nprint (\"db2: \"+ str(grads[\"db2\"]))","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"hGjkm8eLrcRL"},"cell_type":"markdown","source":"**Expected Output**: \n\n<table style=\"width:65%\">\n    <tr>\n        <td style=\"width:15%\"><b>dW1</b></td>\n       <td> [[ 0.02797138 -0.02273175]<br>\n [ 0.02765858  0.02777095]<br>\n [-0.01830467 -0.01926442]<br>\n [ 0.03115285  0.02999027]] </td>\n    </tr>\n    <tr>\n        <td><b>db1</b></td>\n       <td> [[0.00155825]<br>\n [0.00133963]<br>\n [0.00390176]<br>\n [0.0030709 ]]</td>\n    </tr>\n    <tr>\n        <td><b>dW2</b></td>\n       <td> [[-0.27659717 -0.22954308 -0.27481279 -0.2806681 ]<br>\n [-0.22820569 -0.28276153 -0.27739741 -0.28270719]<br>\n [-0.27823467 -0.23104133 -0.27639371 -0.28252569]]<br>\n </td>\n    </tr>\n    <tr>\n        <td><b>db2</b></td>\n       <td> [[-0.50106803]<br>\n [-0.5049884 ]<br>\n [-0.50379404]] </td>\n    </tr>\n</table>"},{"metadata":{"colab_type":"text","id":"XjzoQVKJASHl"},"cell_type":"markdown","source":"Update parameters test:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"cKeHcYA5ASHl","trusted":true},"cell_type":"code","source":"nn_test.update_parameters(grads)\nprint('W1:', nn_test.W1)\nprint('b1:', nn_test.b1)\nprint('W2:', nn_test.W2)\nprint('b2:', nn_test.b2)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"CKWwmm_kASHn"},"cell_type":"markdown","source":"**Expected Output**: \n\n<table style=\"width:60%\">\n    <tr>\n        <td style=\"width:15%\"><b>W1</b></td>\n       <td> [[ 0.00468743 -0.00115533]<br>\n [ 0.0062003   0.01495259]<br>\n [-0.00215849 -0.00214873]<br>\n [ 0.0154806   0.00737444]]</td>\n    </tr>\n    <tr>\n        <td><b>b1</b></td>\n       <td> [[-1.55825086e-05]<br>\n [-1.33963447e-05]<br>\n [-3.90175629e-05]<br>\n [-3.07090141e-05]]</td>\n    </tr>\n    <tr>\n       <td><b>W2</b></td>\n       <td> [[-0.00192877  0.00772103 -0.00188605 -0.00185062]<br>\n [ 0.00470168 -0.01630519 -0.0144752  -0.0027958 ]<br>\n [-0.00734596  0.00545289 -0.0063163  -0.01129778]] </td>\n    </tr>\n    <tr>\n       <td><b>db2</b></td>\n       <td> [[0.00501068]<br>\n [0.00504988]<br>\n [0.00503794]] </td>\n    </tr>\n</table>"},{"metadata":{"colab_type":"text","id":"QPUbpsY5rcRM"},"cell_type":"markdown","source":"### 3.2 - Model ####\n\nNow we can aggregate all previous modules into a classifier:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"lFr66y7ercRM","trusted":true},"cell_type":"code","source":"# GRADED CLASS NNClassifier\n\nclass NNClassifier:\n    \"\"\"\n    NNClassifier class\n    \n    Arguments:\n    model -- instance of NN\n    epochs: int -- Number of epochs\n    \"\"\"\n    def __init__(self, model, epochs=1000):\n        self.model = model\n        self.epochs = epochs\n        self._cost = [] # Write value of cost function after each epoch to build graph later\n    \n    def fit(self, X, Y):\n        \"\"\"\n        Learn weights and errors from training data\n\n        Arguments:\n        X -- input data of shape (number of features, number of examples)\n        Y -- labels of shape (1, number of examples)\n        \"\"\"\n    \n        ### START CODE HERE ### \n        self.model.initialize_parameters()\n        Y_hot = one_hot(Y, self.model.n_classes)\n        for i in range(self.epochs):\n            cache = self.model.forward_propagation(X)\n            grads = self.model.backward_propagation(X, Y_hot, cache)\n            self.model.update_parameters(grads)\n            self._cost.append(compute_cost(cache['A2'], Y_hot))\n\n        ### END CODE HERE ###\n    \n    def predict(self, X):\n        \"\"\"\n        Generate array of predicted labels for the input dataset\n        \n        Arguments:\n        X -- input data of shape (number of features, number of examples)\n        \n        Returns:\n        predicted labels of shape (1, n_samples)\n        \"\"\"\n        \n        ### START CODE HERE ### (≈ 2 lines of code)\n        cache = self.model.forward_propagation(X)\n\n        ### END CODE HERE ###\n\n        return np.argmax(cache['A2'], axis=0).T","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"m_J_Jv9SrcRQ"},"cell_type":"markdown","source":"Accuracy:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"WPvNG1HArcRQ","trusted":true},"cell_type":"code","source":"def accuracy(pred, labels):\n    return (np.sum(pred == labels, axis=1) / float(labels.shape[1]))[0]","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"-2cHUv4CrcRU"},"cell_type":"markdown","source":"Function for visualizing an error change:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"AqhzjlfZrcRU","trusted":true},"cell_type":"code","source":"def plot_error(model, epochs):\n    plt.plot(range(len(model._cost)), model._cost)\n    plt.ylim([0, epochs])\n    plt.ylabel('Error')\n    plt.xlabel('Epochs')\n    plt.show()","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"WqWqxvE7rcRW"},"cell_type":"markdown","source":"## 4 - Training \n\nLet's initialize our classifier:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"D8vjQ1_drcRX","trusted":true},"cell_type":"code","source":"NN = NeuralNetwork(784, 30, 10, 0.01)\nclassifier = NNClassifier(NN, 5000)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"7biLv6NrASHz"},"cell_type":"markdown","source":"After last iteration value of cost function must be <1.8:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"fuey9U4QrcRY","trusted":true},"cell_type":"code","source":"classifier.fit(train_set_x, train_set_y)","execution_count":0,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"HRZl2Yl0rcRa","trusted":true},"cell_type":"code","source":"plot_error(classifier, 10)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"ZU01dCkxrcRa"},"cell_type":"markdown","source":"## 5 - Making predictions"},{"metadata":{"colab_type":"text","id":"d3SV2SXkASH3"},"cell_type":"markdown","source":"Accuracy on the train set must be >0.95, on the test set - >0.86"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"WOpTw9j7rcRb","trusted":true},"cell_type":"code","source":"pred_train = classifier.predict(train_set_x)\npred_test = classifier.predict(test_set_x)\n\nprint('train set accuracy: ', accuracy(pred_train, train_set_y))\nprint('test set accuracy: ', accuracy(pred_test, test_set_y))","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"GA32kXLzrcRd"},"cell_type":"markdown","source":"Now we can check our classifier on single example:"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"jC0-KSTjrcRd","trusted":true},"cell_type":"code","source":"plot_digit(test_set_x, test_set_y, idx=6)","execution_count":0,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"mGSinEG2rcRf","trusted":true},"cell_type":"code","source":"pred_single = classifier.predict(test_set_x.T[6].reshape(784, 1))\nprint(\"The digit is \" + str(pred_single[0]))","execution_count":0,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"QeCRbbkArcRg","trusted":true},"cell_type":"code","source":"plot_digit(test_set_x, test_set_y, idx=90)","execution_count":0,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"eZM_KmWprcRi","trusted":true},"cell_type":"code","source":"pred_single = classifier.predict(test_set_x.T[90].reshape(784, 1))\nprint(\"The digit is \" + str(pred_single[0]))","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"vJo13IZHrcRk"},"cell_type":"markdown","source":"## 7 - Conclusion\nAs we can see, our model fits well the hypothesis function to the data.\n\n#### What's next:\n1. Try to experiment with different parameters to to make prediction better.\n2. Compare the results you have obtained with the `sklearn.neural_network` models.\n3. Try this model in the wild! Select your favorite dataset [here](https://www.kaggle.com/datasets?sortBy=hottest&group=public&page=1&pageSize=20&size=small&filetype=all&license=all&tagids=13303) and play with it."}],"metadata":{"colab":{"collapsed_sections":["R5uVa8VCrcQx","QPUbpsY5rcRM"],"default_view":{},"name":"NN.ipynb","provenance":[],"version":"0.3.2","views":{}},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}